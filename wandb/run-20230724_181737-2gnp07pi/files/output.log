Done.
Setting up the random seed for reproducibility...
Done.
dynamic_import - class_name:  OracleTrainer
Initializing model...experiments third
Done.
Initializing datasets...
Done.
Initializing clients...
dynamic_import - class_name:  OracleClient
Done.
Initializing server...
dynamic_import - class_name:  OracleServer
Running without server optimizer
Done.
Initialize return score, metrics, ckpt, ckpt step...
Setting up metrics...
Done.
Done.
Initializing optimizer and scheduler...
Done.
dynamic_import - class_name:  OracleClient
The experiment begins...
ROUND 1/6: Training 2 Clients...
CLIENT 1/2: erfurt5 MIX
Traceback (most recent call last):
  File "/home/utente/anaconda3/envs/LADD/lib/python3.9/multiprocessing/queues.py", line 251, in _feed
    send_bytes(obj)
  File "/home/utente/anaconda3/envs/LADD/lib/python3.9/multiprocessing/connection.py", line 205, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/utente/anaconda3/envs/LADD/lib/python3.9/multiprocessing/connection.py", line 416, in _send_bytes
    self._send(header + buf)
  File "/home/utente/anaconda3/envs/LADD/lib/python3.9/multiprocessing/connection.py", line 373, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
Traceback (most recent call last):
  File "/home/utente/anaconda3/envs/LADD/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/utente/anaconda3/envs/LADD/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/utente/Scrivania/PROVA/LADD/src/modules/deeplabv3.py", line 30, in forward
    features_rgb = self.rgb_backbone(x_rgb)
  File "/home/utente/anaconda3/envs/LADD/lib/python3.9/site-packages/torch/fx/graph_module.py", line 616, in wrapped_call
    raise e.with_traceback(None)
RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 3.95 GiB total capacity; 2.87 GiB already allocated; 8.12 MiB free; 2.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF